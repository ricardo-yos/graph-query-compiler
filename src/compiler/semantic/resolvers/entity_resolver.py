"""
Entity Resolver
===============

Semantic validation and normalization layer for attributes generated
by an LLM inside a graph-based semantic compiler pipeline.

This component enforces structural and semantic grounding by ensuring
that LLM-proposed attributes:

1. Exist in the canonical graph schema.
2. Belong to a valid node label.
3. Are semantically supported by the user question.

Invalid or hallucinated attributes are removed. Valid attributes are
normalized to their canonical (label, attribute) representation.

Input
-----
question : str
    Natural language user query.

llm_schema : dict
    Schema generated by the LLM.

Output
------
dict
    Dictionary containing:
        - original_schema : Unmodified input schema.
        - resolved_schema : Validated and normalized schema.
        - analysis        : Resolution audit log.
"""

from typing import Dict, Any, List, Tuple, Optional
import copy
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path
from schema.schema_loader import load_schema
from config.paths import SCHEMA_DATA_DIR


class EntityResolver:
    """
    Performs structural and semantic grounding of LLM-generated attributes
    against a predefined graph schema.
    """

    def __init__(
        self,
        model: SentenceTransformer,
        threshold: float = 0.65,
        ngram_range: Tuple[int, int] = (1, 3),
        lexical_boost: float = 0.15
    ):
        """
        Parameters
        ----------
        model : SentenceTransformer
            Embedding model used for semantic similarity computation.
        threshold : float
            Minimum similarity required to keep an attribute.
        ngram_range : tuple
            Range of n-grams used during semantic matching.
        lexical_boost : float
            Constant additive boost applied to semantic similarity.
        """

        self.model = model
        self.threshold = threshold
        self.ngram_range = ngram_range
        self.lexical_boost = lexical_boost

        # Load canonical schema once (strong structural grounding layer)
        self.schema_path = Path(SCHEMA_DATA_DIR) / "graph_schema.json"
        self.graph_schema = load_schema(self.schema_path)

        # Cache embeddings to reduce repeated model inference
        self._embedding_cache: Dict[str, np.ndarray] = {}

    # -----------------------------------------------------
    # Embedding Layer
    # -----------------------------------------------------

    def _embed_batch(self, texts: List[str]) -> np.ndarray:
        """
        Computes normalized embeddings with caching.
        Avoids redundant calls to the embedding model.
        """

        missing = [t for t in texts if t not in self._embedding_cache]

        if missing:
            embeddings = self.model.encode(
                missing,
                normalize_embeddings=True
            )
            for text, emb in zip(missing, embeddings):
                self._embedding_cache[text] = emb

        return np.array([self._embedding_cache[t] for t in texts])

    # -----------------------------------------------------
    # N-gram Generation
    # -----------------------------------------------------

    def _generate_ngrams(self, tokens: List[str]) -> List[str]:
        """
        Generates contiguous n-grams from tokenized input.
        Used for semantic alignment against attribute descriptions.
        """

        ngrams = []
        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):
            for i in range(len(tokens) - n + 1):
                ngrams.append(" ".join(tokens[i:i+n]))
        return ngrams

    # -----------------------------------------------------
    # Schema Lookup
    # -----------------------------------------------------

    def _find_attribute_in_schema(
        self,
        attribute_name: str
    ) -> Optional[Tuple[str, str, str]]:
        """
        Searches globally for an attribute in the graph schema.

        Returns
        -------
        tuple (label, canonical_attribute, description)
            If attribute exists, otherwise None.
        """

        for label, node_data in self.graph_schema.get("nodes", {}).items():
            properties = node_data.get("properties", {})

            if isinstance(properties, dict):
                if attribute_name in properties:
                    desc = properties[attribute_name].get(
                        "description",
                        attribute_name
                    )
                    return label, attribute_name, desc

            elif isinstance(properties, list):
                if attribute_name in properties:
                    return label, attribute_name, attribute_name

        return None

    # -----------------------------------------------------
    # Semantic Similarity
    # -----------------------------------------------------

    def _attribute_similarity(
        self,
        question: str,
        attribute_name: str,
        attribute_desc: str
    ) -> float:
        """
        Computes semantic similarity between the question
        and an attribute (name + description), applying
        a constant lexical boost.

        Returns
        -------
        float
            Final similarity score (clamped to 1.0).
        """

        question_lower = question.lower()

        # Fast lexical grounding shortcut
        if attribute_name.lower() in question_lower:
            return 1.0

        tokens = question_lower.split()
        ngrams = self._generate_ngrams(tokens)

        if not ngrams:
            return 0.0

        texts_to_embed = ngrams + [attribute_name, attribute_desc]
        embeddings = self._embed_batch(texts_to_embed)

        ngram_embs = embeddings[:len(ngrams)]
        attr_embs = embeddings[len(ngrams):]

        semantic_score = float(
            np.max(cosine_similarity(ngram_embs, attr_embs))
        )

        # Controlled lexical reinforcement
        final_score = semantic_score + self.lexical_boost

        return min(final_score, 1.0)

    # -----------------------------------------------------
    # Main Resolver
    # -----------------------------------------------------

    def resolve(
        self,
        question: str,
        llm_schema: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Validates and normalizes attributes proposed by the LLM.

        Process:
        1. Structural validation (label-first).
        2. Ontological fallback lookup.
        3. Semantic grounding validation.
        4. Remove unsupported attributes.
        5. Normalize valid attributes.
        """

        original_schema = copy.deepcopy(llm_schema)
        resolved_schema = copy.deepcopy(llm_schema)

        analysis = []

        filters = resolved_schema.get("constraints", {}).get("filters", [])

        # Iterate over a copy to allow safe in-place removal
        for f in filters[:]:

            attr_name = f.get("attribute")
            llm_label = f.get("label")

            found = None

            # 1 — Structural grounding (label-first validation)
            if llm_label:
                node_data = self.graph_schema.get("nodes", {}).get(llm_label)
                if node_data:
                    properties = node_data.get("properties", {})

                    if isinstance(properties, dict) and attr_name in properties:
                        desc = properties[attr_name].get(
                            "description",
                            attr_name
                        )
                        found = (llm_label, attr_name, desc)

                    elif isinstance(properties, list) and attr_name in properties:
                        found = (llm_label, attr_name, attr_name)

            # 2 — Ontological fallback (global schema search)
            if not found:
                found = self._find_attribute_in_schema(attr_name)

            if not found:
                analysis.append({
                    "attribute": attr_name,
                    "label": llm_label,
                    "kept": False,
                    "reason": "not_in_graph_schema"
                })
                filters.remove(f)
                continue

            repaired_label, canonical_attr, attr_desc = found

            # 3 — Semantic grounding validation
            similarity = self._attribute_similarity(
                question,
                canonical_attr,
                attr_desc
            )

            if similarity < self.threshold:
                analysis.append({
                    "attribute": attr_name,
                    "label": llm_label,
                    "kept": False,
                    "reason": "not_in_question",
                    "similarity": round(similarity, 3)
                })
                filters.remove(f)
            else:
                # Normalize to canonical schema representation
                f["attribute"] = canonical_attr
                f["label"] = repaired_label

                analysis.append({
                    "attribute": attr_name,
                    "original_label": llm_label,
                    "resolved_label": repaired_label,
                    "kept": True,
                    "similarity": round(similarity, 3)
                })

        return {
            "original_schema": original_schema,
            "resolved_schema": resolved_schema,
            "analysis": analysis
        }
